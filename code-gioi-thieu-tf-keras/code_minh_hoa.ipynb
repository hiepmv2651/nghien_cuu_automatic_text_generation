{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Đây là một đoạn code Python sử dụng thư viện nltk để tách câu thành các token riêng lẻ:"
      ],
      "metadata": {
        "id": "KTtBjI96kGcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgVBSq1BkDih",
        "outputId": "950842ab-1509-4b4a-cdc5-64992a2e8c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chúng', 'ta', 'đang', 'học', 'về', 'Xử', 'lý', 'Ngôn', 'ngữ', 'Tự', 'nhiên', '!']\n",
            "['Chúng', 'ta', 'đang', 'học', 'về', 'Xử', 'lý', 'Ngôn', 'ngữ', 'Tự', 'nhiên']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Chúng ta đang học về Xử lý Ngôn ngữ Tự nhiên!\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu muốn giữ lại cụm từ \"Xử lý Ngôn ngữ Tự nhiên\" như là một token duy nhất, ta có thể sử dụng các ký tự đặc biệt như \"_\" để giữ lại cụm từ này khi Tokenization:"
      ],
      "metadata": {
        "id": "2w3Hufi3lveQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sentence = \"Chúng ta đang học về Xử lý Ngôn ngữ Tự nhiên!\"\n",
        "tokens = sentence.split()\n",
        "new_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    if len(re.findall('[a-zA-Z]+', token)) > 0:\n",
        "        new_token = re.sub('\\W+', '_', token)\n",
        "        new_tokens.append(new_token)\n",
        "\n",
        "new_sentence = ' '.join(new_tokens)\n",
        "result = new_sentence.split()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1rKNv1Ylw41",
        "outputId": "bc451f51-eae1-4633-81eb-8abee019fde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chúng', 'ta', 'đang', 'học', 'về', 'Xử', 'lý', 'Ngôn', 'ngữ', 'Tự', 'nhiên_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để minh họa cho việc tạo Word Embedding bằng mã nguồn, ta sử dụng thư viện Gensim trong Python. Đoạn code sau đây mô tả cách sử dụng Gensim để tạo Word Embedding từ một văn bản:"
      ],
      "metadata": {
        "id": "9f_kmsV1qHmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Tải văn bản\n",
        "nltk.download('punkt')\n",
        "text = \"Tôi rất thích ăn kem vào mùa hè\"\n",
        "\n",
        "# Chia câu và từ\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# Tạo Word2Vec model\n",
        "model = Word2Vec(words, min_count=1)\n",
        "\n",
        "# Truy cập vector của từ 'kem'\n",
        "vector = model.wv['kem']\n",
        "\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCksKkSzqJVr",
        "outputId": "46ecaa99-0c4f-419d-c047-95bdfbf92706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là đoạn code minh họa về việc sử dụng phương pháp mã hóa one-hot cho các từ trong văn bản tiếng Việt, trong đó từ \"sorbet\" được thay thế bằng từ \"kem trái cây\":"
      ],
      "metadata": {
        "id": "wCH4R_Tdup8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tập từ điển của văn bản\n",
        "vocab = [\"kem\", \"trái cây\", \"thật\", \"ngon\"]\n",
        "\n",
        "# Văn bản cần biểu diễn\n",
        "text = \"kem trái cây thật ngon\"\n",
        "\n",
        "# Khởi tạo ma trận biểu diễn theo one-hot encoding\n",
        "one_hot_matrix = np.zeros((len(text.split()), len(vocab)))\n",
        "\n",
        "# Biểu diễn các từ trong văn bản thành vector one-hot tương ứng\n",
        "for i, word in enumerate(text.split()):\n",
        "    if word in vocab:\n",
        "        one_hot_matrix[i][vocab.index(word)] = 1\n",
        "\n",
        "# In ra ma trận biểu diễn theo one-hot encoding\n",
        "print(one_hot_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plt9zczyuqv9",
        "outputId": "5bfdec89-201d-40f2-9485-204100bccc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}