{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "q4Qd1scqvJ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cài đặt thư viện\n",
        "!pip install pyvi"
      ],
      "metadata": {
        "id": "Qn62WMEqtMWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcDWIMvFpbzI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import chardet\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvi import ViTokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Bidirectional, Dense, BatchNormalization\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Tải lên tệp văn bản\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Chuẩn bị dữ liệu\n",
        "data = \"\"\n",
        "for filename in uploaded.keys():\n",
        "    with open(filename, \"rb\") as f:\n",
        "        encoding = chardet.detect(f.read())['encoding']\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        data += f.read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# Lưu kho văn bản vào tập tin\n",
        "corpus_filename = 'corpus.txt'\n",
        "with open(corpus_filename, 'w') as f:\n",
        "    f.write('\\n'.join(corpus))\n",
        "\n",
        "!cp corpus.txt \"/content/drive/My Drive/\"\n",
        "\n",
        "# Xác định một hàm để tiền xử lý dữ liệu văn bản.\n",
        "def preprocess_text(text):\n",
        "    # Loại bỏ đường dẫn URL, tên người dùng và các từ khóa trong hashtag\n",
        "    text = re.sub(r'(https?:\\/\\/[^\\s]*)|(www\\.[^\\s]*)|(@[^\\s]*)|(#\\w+)', '', text)\n",
        "    # Chuyển đổi chữ hoa thành chữ thường trong văn bản\n",
        "    text = text.lower()\n",
        "    # Ghép từ tiếng Việt\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "    # Xóa dấu câu và ký tự thừa\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"_\", \"\")))\n",
        "    # Trả lại văn bản được xử lý trước\n",
        "    return text\n",
        "\n",
        "# Tiền xử lý dữ liệu văn bản và mã hóa các từ\n",
        "preprocessed_lines = [preprocess_text(line) for line in corpus]\n",
        "\n",
        "# Chia dữ liệu thành 70% train và 30% test\n",
        "train_data, test_data = train_test_split(preprocessed_lines, test_size=0.3, random_state=42)\n",
        "\n",
        "# Tiền xử lý dữ liệu cho tập train\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in train_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "predictors_train, label_train = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label_train = tf.keras.utils.to_categorical(label_train, num_classes=total_words)\n",
        "\n",
        "# Tiền xử lý dữ liệu cho tập test\n",
        "test_sequences = []\n",
        "for line in test_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        test_sequences.append(n_gram_sequence)\n",
        "\n",
        "test_sequences = np.array(pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "predictors_test, label_test = test_sequences[:, :-1], test_sequences[:, -1]\n",
        "label_test = tf.keras.utils.to_categorical(label_test, num_classes=total_words)\n",
        "\n",
        "# Tạo dataset cho việc train và test mô hình\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((predictors_train, label_train)).shuffle(len(predictors_train)).batch(128)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((predictors_test, label_test)).batch(128)\n",
        "\n",
        "# Xây dựng mô hình LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(total_words, 100),\n",
        "    Bidirectional(LSTM(150, return_sequences=True)),\n",
        "    BatchNormalization(),\n",
        "    LSTM(100),\n",
        "    Dropout(0.2),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Sử dụng EarlyStopping callback để dừng quá trình train sớm hơn nếu không có sự cải thiện đáng kể về độ chính xác\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Sử dụng ModelCheckpoint callback để lưu lại mô hình tốt nhất trong quá trình train\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/My Drive/best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_dataset, epochs=100, validation_data=test_dataset, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Đánh giá mô hình trên tập test\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "# Lưu lại mô hình\n",
        "model_filename = 'model.h5'\n",
        "model.save(model_filename)\n",
        "\n",
        "!cp model.h5 \"/content/drive/My Drive/\"\n",
        "\n",
        "with open('/content/drive/My Drive/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "np.save('/content/drive/MyDrive/sequences.npy', input_sequences)\n",
        "\n",
        "# Vẽ đồ thị biểu diễn sự biến thiên của hàm mất mát trong quá trình huấn luyện và đánh giá\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "ax.plot(history.history['loss'], label='train')\n",
        "ax.plot(history.history['val_loss'], label='val')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training Loss Curve')\n",
        "ax.legend()\n",
        "plt.savefig('/content/drive/MyDrive/train_loss.png')\n",
        "\n",
        "# Vẽ đồ thị biểu diễn sự biến thiên của độ chính xác trong quá trình huấn luyện và đánh giá\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "ax.plot(history.history['accuracy'], label='train')\n",
        "ax.plot(history.history['val_accuracy'], label='val')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Training Accuracy Curve')\n",
        "ax.legend()\n",
        "plt.savefig('/content/drive/MyDrive/train_acc.png')\n",
        "\n",
        "# Vẽ đồ thị kiến trúc mô hình\n",
        "plot_model(model, to_file='/content/drive/MyDrive/model_structure.png', show_shapes=True, rankdir='TB')\n",
        "\n",
        "# Đánh giá mô hình trên tập test\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train tiếp model"
      ],
      "metadata": {
        "id": "YS1jktgypkti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import chardet\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvi import ViTokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "import os\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Đặt các đường dẫn\n",
        "root_path = '/content/drive/My Drive/'\n",
        "model_path = os.path.join(root_path, 'model.h5')\n",
        "tokenizer_path = os.path.join(root_path, 'tokenizer.pkl')\n",
        "corpus_path = os.path.join(root_path, 'corpus.txt')\n",
        "\n",
        "# Tải mô hình và công cụ tokenizer đã được đào tạo trước\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Tải lên tệp dữ liệu văn bản mới\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Thêm dữ liệu mới vào ngữ liệu và tiền xử lý\n",
        "data = \"\"\n",
        "for filename in uploaded.keys():\n",
        "    with open(filename, \"rb\") as f:\n",
        "        encoding = chardet.detect(f.read())['encoding']\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        data += f.read()\n",
        "new_corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# Kết hợp dữ liệu cũ và mới\n",
        "if os.path.exists(corpus_path):\n",
        "    with open(corpus_path, 'r') as f:\n",
        "        corpus = f.read().lower().split('\\n')\n",
        "else:\n",
        "    corpus = []\n",
        "corpus += new_corpus\n",
        "corpus = list(filter(None, corpus))\n",
        "\n",
        "# Xác định một hàm để tiền xử lý dữ liệu văn bản.\n",
        "def preprocess_text(text):\n",
        "    # Loại bỏ đường dẫn URL, tên người dùng và các từ khóa trong hashtag\n",
        "    text = re.sub(r'(https?:\\/\\/[^\\s]*)|(www\\.[^\\s]*)|(@[^\\s]*)|(#\\w+)', '', text)\n",
        "    # Chuyển đổi chữ hoa thành chữ thường trong văn bản\n",
        "    text = text.lower()\n",
        "    # Ghép từ tiếng Việt\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "    # Xóa dấu câu và ký tự thừa\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"_\", \"\")))\n",
        "    # Trả lại văn bản được xử lý trước\n",
        "    return text\n",
        "\n",
        "preprocessed_lines = [preprocess_text(line) for line in new_corpus]\n",
        "\n",
        "# Chia dữ liệu thành 70% train và 30% test\n",
        "train_data, test_data = train_test_split(preprocessed_lines, test_size=0.3, random_state=42)\n",
        "\n",
        "tokenizer.fit_on_texts(new_corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Lấy kích thước của lớp cuối cùng và cập nhật nó\n",
        "final_layer_shape = model.layers[-1].output_shape\n",
        "new_output_layer = Dense(total_words, activation='softmax')(model.layers[-2].output)\n",
        "model.layers[-1] = Dense(total_words, activation='softmax')\n",
        "model = tf.keras.models.Model(inputs=model.input, outputs=new_output_layer)\n",
        "\n",
        "# Tiền xử lý dữ liệu cho tập train\n",
        "input_sequences = []\n",
        "for line in train_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = to_categorical(labels, num_classes=total_words)\n",
        "train_data, test_data, train_label, test_label = train_test_split(xs, ys, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# predictors_train, label_train = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "label_train = to_categorical(labels)\n",
        "\n",
        "# Tiền xử lý dữ liệu cho tập test\n",
        "test_sequences = []\n",
        "for line in test_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        test_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_len = max([len(x) for x in test_sequences])\n",
        "test_sequences = np.array(pad_sequences(test_sequences, maxlen=max_len, padding='pre'))\n",
        "# test_sequences = np.array(pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "test_xs, test_labels = test_sequences[:,:-1],test_sequences[:,-1]\n",
        "test_ys = to_categorical(test_labels, num_classes=total_words)\n",
        "\n",
        "# predictors_test, label_test = test_sequences[:, :-1], test_sequences[:, -1]\n",
        "# label_test = to_categorical(label_test, num_classes=total_words)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks=[EarlyStopping(patience=5, monitor='val_loss'),\n",
        "ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss')]\n",
        "history = model.fit(train_data, train_label, epochs=100, verbose=1, validation_data=(test_xs, test_ys), callbacks=callbacks)\n",
        "# history = model.fit(xs, ys, epochs=100, verbose=1, validation_data=(test_xs, test_ys), callbacks=callbacks)\n",
        "\n",
        "# Define early stopping and model checkpoint callbacks\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
        "# model_checkpoint = ModelCheckpoint(os.path.join(root_path, 'best_model.h5'), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "# Retrain the model with early stopping and model checkpoint callbacks\n",
        "# with tf.device('/device:GPU:0'):\n",
        "#     history = model.fit(predictors_train, label_train, epochs=100, verbose=1, validation_data=(predictors_test, label_test), callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Đánh giá mô hình trên tập test\n",
        "# test_loss, test_acc = model.evaluate(predictors_test, label_test)\n",
        "test_loss, test_acc = model.evaluate(test_xs, test_ys)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "# Lưu mô hình và công cụ tokenizer đã được cập nhật\n",
        "model.save(model_path)\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "  pickle.dump(tokenizer, f)\n",
        "\n",
        "# Lưu lại dữ liệu đã được cập nhật\n",
        "with open(corpus_path, 'w') as f:\n",
        "  f.write('\\n'.join(corpus))\n",
        "\n",
        "# Vẽ đồ thị biểu diễn sự biến thiên của hàm mất mát trong quá trình huấn luyện\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Lưu đồ thị vào Google Drive\n",
        "plt.savefig(os.path.join(root_path, 'loss.png'))\n",
        "\n",
        "# Vẽ đồ thị biểu diễn sự biến thiên của độ chính xác trong quá trình huấn luyện\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Lưu đồ thị vào Google Drive\n",
        "plt.savefig(os.path.join(root_path, 'accuracy.png'))\n",
        "\n",
        "# Vẽ đồ thị kiến trúc mô hình\n",
        "plot_model(model, to_file='/content/drive/MyDrive/model_structure.png', show_shapes=True, rankdir='TB')\n",
        "\n"
      ],
      "metadata": {
        "id": "eyt5bS2IyvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tính điểm BLEU"
      ],
      "metadata": {
        "id": "XE_h921BvQZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Tải mô hình và công cụ tokenizer được đào tạo trước từ tệp pickle\n",
        "root_path = '/content/drive/My Drive/'\n",
        "model_path = os.path.join(root_path, 'model.h5')\n",
        "tokenizer_path = os.path.join(root_path, 'tokenizer.pkl')\n",
        "\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Tạo ra văn bản bằng cách sử dụng mô hình đã được đào tạo trước\n",
        "seed_text = \"bài báo\"\n",
        "next_words = 11\n",
        "generated_text = seed_text\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=model.input_shape[1], padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "    generated_text += \" \" + output_word\n",
        "\n",
        "# Đánh giá văn bản được tạo ra bằng cách sử dụng điểm BLEU với kỹ thuật smoothing\n",
        "reference_text = [\"bài báo nêu rõ các vấn đề của nền kinh tế Việt Nam\"]\n",
        "\n",
        "hypothesis_text = generated_text.split()\n",
        "reference_text = reference_text[0].split()\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_score = sentence_bleu([reference_text], hypothesis_text, smoothing_function=smoothie)\n",
        "\n",
        "# In ra văn bản được tạo ra và điểm BLEU tương ứng của nó\n",
        "print(\"Generated text:\", generated_text)\n",
        "print(\"reference text:\", ' '.join(reference_text))\n",
        "print(\"BLEU score:\", bleu_score)"
      ],
      "metadata": {
        "id": "GV_l4WNxGelb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}